# Vision Library ðŸ‘€

## Introduction

A library that leverages advanced computer vision techniques to accurately detect and interpret human gestures in real-time, addressing one of the
key challenges in human-robot interaction from the perspective of intelligent robot assistants, creating a seamless communication interface between humans and robot assistants.

## Features

The vision library includes the following features:
- Off the shelf **Gesture Recognition** of 7 Pre-Trained gestures.
    - Open Palm
    - Closed Fist
    - Thumb Up
    - Thumb Down
    - Pointing Up
    - Victory
    - I Love You (Sign Language)
- **Custom Gesture Learning** in Real-Time.
- **Hybrid Recognition pipeline** that polls the inferences made by the Pre-Trained model and Custom Classifier.
- **Contextual Natural Language** responses for the detected gestures to enhance the interactions.
- Runs on **local LLM's** to minimize the latency.
- Tailored to the Pepper Humanoid-Robot to generate animated speech responses. Scope for mapping Choregraphe animations to Pre-Trained and Custom Gestures.

## Demo and Dissertation Report

To have a look at the demo please click [here](https://drive.google.com/file/d/1h-v_JkG5j-MQ3RtXYw6h8rlQaTwnwVFS/view?usp=sharing)
To have a look at the in-depth technical dissertation report please click [here](https://drive.google.com/file/d/1QdT9XZFPpBnfPUjKUaokwlsXQ75-Dt8R/view?usp=sharing)
